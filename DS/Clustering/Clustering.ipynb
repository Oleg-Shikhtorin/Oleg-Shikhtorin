{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, r2_score\n",
    "from catboost import Pool, CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../../data/input/'\n",
    "output_path = '../../data/output/'\n",
    "input_filename = 'marketing_campaign.csv'\n",
    "\n",
    "report_dttm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data is coming from https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_path+input_filename, sep='\\t')\n",
    "print(df.shape)\n",
    "print(np.round(df.memory_usage(deep=True).sum() / 1024**2, 2))\n",
    "df.head()\n",
    "\n",
    "# df.to_parquet(f'{output_path}customer_segmentation.parquet.gzip', compression='gzip')\n",
    "# df = pd.read_parquet(f'{output_path}customer_segmentation.parquet.gzip', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df.isnull().sum(),df.dtypes], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARRAY-type columns come from BQ as an 'object', so it's necessary to specify them manually\n",
    "# if there are some\n",
    "\n",
    "array_cols_list = [] # ban_type\n",
    "print('All columns are in the DataFrame: ', len(array_cols_list) > 0 and len(set(array_cols_list).intersection(set(df.columns))) == len(array_cols_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format=\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace empty strings with None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# replaces empty string \"\" with NULL\n",
    "for col in df.select_dtypes(include='O').columns:\n",
    "    df[col] = df[col].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    \n",
    "print(np.round(df.memory_usage(deep=True).sum() / 1024**2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# deletes entirely blank columns\n",
    "empty_cols = list(set(df.columns) - set(df.dropna(axis=1, how='all').columns))\n",
    "one_val_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
    "if empty_cols:\n",
    "    print('These columns only contain NULLs:\\n', ', '.join(empty_cols))\n",
    "    df = df.drop(empty_cols, axis=1)\n",
    "\n",
    "# deletes the columns consisting of only one value\n",
    "if one_val_cols:\n",
    "    print('These columns contain a single value:\\n', ', '.join(one_val_cols))\n",
    "    df = df.drop(one_val_cols, axis=1)\n",
    "\n",
    "print(np.round(df.memory_usage(deep=True).sum() / 1024**2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datatypes optimization, reducing memory usage *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtype_limits(type_name, subtype_name):\n",
    "    \"\"\"\n",
    "    returns min and max values according to the data type\n",
    "    \"\"\"\n",
    "    subtype_name = subtype_name.lower()\n",
    "    if 'int' in type_name.lower():\n",
    "        return (np.iinfo(subtype_name).min, np.iinfo(subtype_name).max)\n",
    "    elif 'float' in type_name.lower():\n",
    "        return (np.finfo(subtype_name).min, np.finfo(subtype_name).max)\n",
    "\n",
    "\n",
    "def reduce_memory(data):\n",
    "    df = data.copy()\n",
    "    print(np.round(df.memory_usage(deep=True).sum() / 1024**2, 2))\n",
    "    subtypes_dict = {\n",
    "        'int64': ['UInt8','UInt16','UInt32','UInt64','Int8','Int16','Int32'],\n",
    "        'float64': ['float16','float32'],\n",
    "        'object': ['category']\n",
    "    }\n",
    "\n",
    "    for col in df.select_dtypes(include='number').columns:\n",
    "        col_type = str(df[col].dtypes).lower()\n",
    "        min_column_value = df[col].min()\n",
    "        max_column_value = df[col].max()\n",
    "\n",
    "        for subtype in subtypes_dict[col_type]:\n",
    "            min_subtype_value, max_subtype_value = get_subtype_limits(col_type, subtype)\n",
    "            if min_column_value>min_subtype_value and max_column_value<max_subtype_value:\n",
    "                df[col] = df[col].astype(subtype)\n",
    "                break\n",
    "\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "            if not type(df.loc[data[col].notnull().index[0],col]) in (np.ndarray, list):\n",
    "                df[col] = df[col].astype('category')\n",
    "    print(np.round(df.memory_usage(deep=True).sum() / 1024**2, 2))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df_reduced = reduce_memory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.to_pickle(f'{output_path}df_reduced.pkl')\n",
    "df_reduced = pd.read_pickle(f'{output_path}df_reduced.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing datatypes to numeric (except \"object\"/\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dttm = df['Dt_Customer'].max().to_pydatetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_date_to_int(data, report_dttm=report_dttm, unit_muliplier=31):\n",
    "    \"\"\"\n",
    "    translating a date into the number of months (days * unit_muliplier) from that date\n",
    "    \"\"\"\n",
    "    dttm_cols = data.select_dtypes(include=['datetime64[ns, UTC]','datetime64[ns]']).columns\n",
    "    if dttm_cols.tolist():\n",
    "        for col in dttm_cols:\n",
    "            data[col] = ((pd.to_datetime(report_dttm.date()) - df['Dt_Customer']).dt.days / unit_muliplier).astype('int64')\n",
    "\n",
    "    # dt_cols = data.drop(dttm_cols, axis=1).select_dtypes(include='dbdate').columns\n",
    "    # for col in dt_cols:\n",
    "    #     data[col] = (report_dttm.date() - pd.to_datetime(data[col]).dt.date).dt.days\n",
    "    print('df_date_to_int: Done')\n",
    "\n",
    "def df_bool_to_int(data):\n",
    "    \"\"\"\n",
    "    translating a bool into the number\n",
    "    \"\"\"\n",
    "    cols = data.select_dtypes(include='bool').columns\n",
    "    if cols.tolist():\n",
    "        for col in cols:\n",
    "            data[col] = data[col].astype('UInt8')\n",
    "    print('df_bool_to_int: Done')\n",
    "\n",
    "def df_array_cols_parse(data, array_columns_list=None):\n",
    "    \"\"\"\n",
    "    creates separate columns from array elements\n",
    "    assigns 1 to the value in the column if there is a corresponding element in the array, otherwise 0\n",
    "    \"\"\"\n",
    "    if array_columns_list:\n",
    "        for col in array_columns_list:\n",
    "            unique_elements_list = list(set([x for xs in data[col].tolist() for x in xs if x.strip() != '']))\n",
    "            for elem in unique_elements_list:\n",
    "                col_name = f'is_{elem}_ban_type'\n",
    "                data[col_name] = 0\n",
    "                idx_to_mark = data[data[col].apply(lambda _: elem in _)].index\n",
    "                data.loc[idx_to_mark, col_name] = 1\n",
    "        data.drop(array_columns_list, axis=1, inplace=True)\n",
    "    print('df_array_cols_parse: Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df_date_to_int(df)\n",
    "df_bool_to_int(df)\n",
    "df_array_cols_parse(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 options\n",
    "feature='Year_Birth'\n",
    "\n",
    "# based on IQR\n",
    "# Q1 = df[feature].quantile(.25)\n",
    "# Q3 = df[feature].quantile(.75)\n",
    "# IQR = Q3 - Q1\n",
    "# bounds = (Q1-1.5*IQR, Q3+1.5*IQR)\n",
    "\n",
    "# based on cutting 2% from each side\n",
    "bounds = df[feature].quantile([.02,.98]).values\n",
    "\n",
    "# replace with None, fill it later\n",
    "df.loc[df[(df[feature]<=min(bounds)) | (df[feature]>=max(bounds))].index, feature] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df.drop(columns='ID').select_dtypes(include='number').corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict = {'column1':[], 'column2':[], 'corr_coef':[]}\n",
    "thrshld = .85 # threshold after which we consider columns as highly correlated\n",
    "\n",
    "for col in corr.columns:\n",
    "    high_corr_ftrs = corr[abs(corr[col]) >= thrshld].index\n",
    "    if list(high_corr_ftrs):\n",
    "        for col2 in high_corr_ftrs:\n",
    "            if col2 != col and col2 not in corr_dict['column1']:\n",
    "                corr_coef = np.round(corr.loc[col2, col] * 100,2)\n",
    "                corr_dict['column1'].append(col)\n",
    "                corr_dict['column2'].append(col2)\n",
    "                corr_dict['corr_coef'].append(corr_coef)\n",
    "\n",
    "pd.DataFrame(corr_dict).sort_values('corr_coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_hide = set(corr_dict['column2'])\n",
    "df.drop(columns=cols_to_hide, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with NULLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replacing with a zero/ average/ mode/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,5,figsize=(15, 4), sharey=True)\n",
    "feature = 'Income'\n",
    "\n",
    "for _ in range(0,5):\n",
    "    ax[_].grid(axis='y')\n",
    "\n",
    "sns.kdeplot(df[feature], ax=ax[0])\n",
    "sns.kdeplot(df[feature].fillna(0), ax=ax[1])\n",
    "sns.kdeplot(df[feature].fillna(df[feature].mean()), ax=ax[2])\n",
    "sns.kdeplot(df[feature].fillna(df[feature].median()), ax=ax[3])\n",
    "sns.kdeplot(df[feature].fillna(df[feature].mode()[0]), ax=ax[4])\n",
    "\n",
    "ax[0].title.set_text('Original')\n",
    "ax[1].title.set_text('Zero')\n",
    "ax[2].title.set_text('Average')\n",
    "ax[3].title.set_text('Median')\n",
    "ax[4].title.set_text('Mode');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filling_dict = {\n",
    "    'mean':['Income'],\n",
    "    'mode':[],\n",
    "    'zero':[],\n",
    "    'max':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_fill_null(df, filling_dict, max_value=9999):\n",
    "    data = df.copy()\n",
    "    \"\"\"\n",
    "    fills NULLs with zeroes/ average/ mode/ etc.\n",
    "    \"\"\"\n",
    "    if 'mean' in filling_dict.keys() and filling_dict['mean']:\n",
    "        cols_to_mean = list(set(filling_dict['mean']).intersection(set(data.columns)))\n",
    "        if cols_to_mean:\n",
    "            for col in cols_to_mean:\n",
    "                mean = data[col].astype('float64').mean()\n",
    "                if np.isnan(mean):\n",
    "                    mean = round(data[col].astype('float64').mean())\n",
    "                else:\n",
    "                    mean = round(mean)\n",
    "                data[col] = data[col].fillna(mean)\n",
    "\n",
    "    if 'mode' in filling_dict.keys() and filling_dict['mode']:\n",
    "        cols_to_mode = list(set(filling_dict['mode']).intersection(set(data.columns)))\n",
    "        if cols_to_mode:\n",
    "            for col in cols_to_mode:\n",
    "                mode = data[col].mode()\n",
    "                if mode.size > 0:\n",
    "                    data[col] = data[col].fillna(mode[0])\n",
    "                else:\n",
    "                    print(f'Column {col} has no mode!')\n",
    "\n",
    "    if 'zero' in filling_dict.keys() and filling_dict['zero']:\n",
    "        cols_to_zero = list(set(filling_dict['zero']).intersection(set(data.columns)))\n",
    "        if cols_to_zero:\n",
    "            data = data.fillna(dict(zip(cols_to_zero,[0]*len(cols_to_zero))))\n",
    "\n",
    "    if 'max' in filling_dict.keys() and filling_dict['max']:\n",
    "        cols_to_max = list(set(filling_dict['max']).intersection(set(data.columns)))\n",
    "        if cols_to_max:\n",
    "            data = data.fillna(dict(zip(cols_to_max,[max_value]*len(cols_to_max))))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_fill_null(df, filling_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filling with ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_fill = 'Income' # one of the columns containing NULLs to be filled\n",
    "temp_df = pd.concat([\n",
    "    df[df.columns[~df.isnull().any()]], # columns with no NULLs\n",
    "    df[col_to_fill] \n",
    "], axis=1).drop(columns=['ID'])\n",
    "\n",
    "category_cols = temp_df.select_dtypes(include='category').columns.tolist()\n",
    "if category_cols:\n",
    "    for col in tqdm(category_cols):\n",
    "        temp_df[col] = temp_df[col].astype(str)\n",
    "\n",
    "df_notnull = temp_df[temp_df[col_to_fill].notnull()]\n",
    "df_null = temp_df[temp_df[col_to_fill].isnull()].drop(columns=col_to_fill)\n",
    "\n",
    "print(df_notnull.shape)\n",
    "print(df_null.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df_notnull.drop(columns=col_to_fill), df_notnull[col_to_fill]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "cat_features = X.select_dtypes(include='O').columns.tolist()\n",
    "\n",
    "train_pool = Pool(data=X_train, label=y_train, cat_features=cat_features)\n",
    "test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)\n",
    "fill_pool = Pool(data=df_null, cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(\n",
    "    iterations=10,\n",
    "    depth=2,\n",
    "    learning_rate=.5,\n",
    "    loss_function='RMSE',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.fit(train_pool)\n",
    "y_pred = model.predict(test_pool)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df_null.index, 'Income'] = model.predict(fill_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.drop('ID', axis=1), corner=True, plot_kws = {'s': 1, 'alpha': 0.3});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=pd.qcut(df['Income'], 6, duplicates='drop').astype(str), y=df['Teenhome'], s=5, alpha=.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['Teenhome'], hue=pd.qcut(df['Income'], 6, duplicates='drop'), showfliers=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=df['Income'], x=df['Dt_Customer'], showfliers=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data to categories (segments) *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "elements_thrshld = 8 # don't split to segments columns having less than <elements_thrshld> unique elements\n",
    "n_segments = 5 # how many segments we want the data to be split to\n",
    "\n",
    "cols_to_segment = [col for col in df.drop(columns=['ID'])._get_numeric_data().columns if df[col].nunique() > elements_thrshld]\n",
    "for col in cols_to_segment:\n",
    "    new_colname = f'{col}_segmented'\n",
    "    df[new_colname] = pd.cut(df[col], n_segments).astype('category')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=cols_to_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_ord_encoding(data, encoder):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
    "    \"\"\"\n",
    "    cols_to_encode = data.select_dtypes(include=['category','object']).columns\n",
    "    new_colnames = [col.replace('_segmented','') + '_encoded' for col in data.select_dtypes(include=['category','object']).columns.tolist()]\n",
    "    data.loc[:,new_colnames] = encoder.fit_transform(data.loc[:,cols_to_encode])\n",
    "    coding_dict = dict(zip(new_colnames, [{idx: value for idx,value in enumerate(_)} for _ in encoder.categories_]))\n",
    "    return cols_to_encode, new_colnames, coding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cats_encoding(data):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    \"\"\"\n",
    "    cols_to_encode = data.select_dtypes(include=['category','object']).columns\n",
    "    encoder = OneHotEncoder(dtype='int32')\n",
    "    res = encoder.fit_transform(data[cols_to_encode])\n",
    "    new_cols = [x for xs in [colname.replace('segmented','')+'_'+category for colname, category in zip(cols_to_encode, encoder.categories_)] for x in xs]\n",
    "    return pd.concat([data, pd.DataFrame(res.toarray(), columns=new_cols)], axis=1).drop(columns=cols_to_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = df_cats_encoding(df.reset_index(drop=True))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_encoder = OrdinalEncoder(dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "original_columns, encoded_columns, coding_dict = df_ord_encoding(df, ord_encoder)\n",
    "df = df.drop(columns=original_columns)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features\n",
    "\n",
    "print(df.shape)\n",
    "print(np.round(df.memory_usage(deep=True).sum() / 1024**2, 2))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats, checks after encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No NULLs:\\t\\t\\t', df.isnull().sum().sum() == 0)\n",
    "print('All columns are numeric:\\t', set(df.columns) == set(df._get_numeric_data().columns))\n",
    "print('DataFrame shape:\\t\\t', df.shape)\n",
    "print('Memory usage:\\t\\t\\t', df.memory_usage(deep=True).sum()/1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# sampling (optional)\n",
    "# df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = scaler.fit_transform(df.drop(columns=['ID']))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving X matrix\n",
    "np.save(f'{output_path}clustering_x.npy', X)\n",
    "\n",
    "# saving preprocessed dataframe\n",
    "df.to_pickle(f'{output_path}df_preproc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xticks(np.arange(0, 100.5, 10))\n",
    "ax.set_xticks(np.arange(0, 100.5, 5), minor=True)\n",
    "ax.set_yticks(np.arange(.5, 1.05, .1))\n",
    "ax.set_yticks(np.arange(.5, 1.05, .05), minor=True)\n",
    "ax.grid(which='both')\n",
    "ax.grid(which='minor', alpha=0.2)\n",
    "ax.grid(which='major', alpha=0.6)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('PCA')\n",
    "plt.xlabel('N components')\n",
    "plt.ylabel('Cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "\n",
    "X = PCA(n_components).fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model_nm = 'kmeans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_clusters = 11\n",
    "\n",
    "# Within-Cluster Sum of Squares\n",
    "wcss = []\n",
    "for i in tqdm(range(1, max_clusters)):\n",
    "    km = KMeans(\n",
    "        n_clusters = i,\n",
    "        init = 'k-means++',\n",
    "        random_state=123\n",
    "    )\n",
    "    km.fit(X)\n",
    "    wcss.append(km.inertia_)\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = (10, 7)\n",
    "plt.plot(range(1, max_clusters), wcss)\n",
    "plt.plot(range(1, max_clusters), wcss, 'o', linewidth=.2, color='black')\n",
    "plt.grid()\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('N clusters')\n",
    "plt.ylabel('WCSS');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = np.diff(wcss) / wcss[:-1] * 100\n",
    "for nm, diff in enumerate(differences):\n",
    "    print(nm+2, '\\t', np.round(diff,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=123)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# save/load centroids (optional)\n",
    "# np.save(output_path+'centroids.npy', kmeans.cluster_centers_)\n",
    "# centroids = np.load(output_path+'centroids.npy', allow_pickle=True)\n",
    "# kmeans = KMeans(n_clusters=n_clusters, random_state=123, init=centroids, max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = f'{model_nm}_{n_clusters}'\n",
    "df[column_name] = kmeans.labels_\n",
    "df[column_name] = df[column_name].astype(str)\n",
    "# df.drop(columns=column_name, inplace=True)\n",
    "\n",
    "df.to_pickle(f'{output_path}df_labeled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_res = pca.fit_transform(X)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(pca_res[:,0], pca_res[:,1], c=kmeans.labels_, s=5, cmap='viridis')\n",
    "plt.title('PCA');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(kmeans.labels_).value_counts().sort_index() # .to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(kmeans.labels_).value_counts(normalize=True).sort_index()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "with open(f'{output_path}Model_segmentation_{model_nm}_{n_clusters}.pkl','wb') as f:\n",
    "    pickle.dump(kmeans,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe (preprocessed filtered data)\n",
    "df = pd.read_pickle(f'{output_path}df_preproc.pkl')\n",
    "print('DataFrame:', df.shape)\n",
    "\n",
    "# X (scaled matrix)\n",
    "X = np.load(f'{output_path}clustering_x.npy', allow_pickle=True)\n",
    "print('X:', X.shape)\n",
    "\n",
    "# load model\n",
    "with open(f'{output_path}Model_segmentation_{model_nm}_{n_clusters}.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    print('Loading the model: Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labels = model.predict(X)\n",
    "\n",
    "column_name = f'{model_nm}_{n_clusters}'\n",
    "df[column_name] = model_labels\n",
    "df[column_name] = df[column_name].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = f'{output_path}df_labeled.pkl'\n",
    "output_filename = f'{output_path}feature_importances_{n_clusters}_labeled_{model_nm}.csv'\n",
    "print('input_filename:\\t\\t',input_filename)\n",
    "print('output_filename:\\t',output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(input_filename).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = df.select_dtypes(include=['category','object']).columns.tolist()\n",
    "if category_cols:\n",
    "    for col in tqdm(category_cols):\n",
    "        df[col] = df[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MultiClass (not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "column_name = f'{model_nm}_{n_clusters}'\n",
    "X, y = df.drop(columns=[column_name]), df[column_name]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "cat_features = X.select_dtypes(include='O').columns.tolist()\n",
    "\n",
    "train_pool = Pool(data=X_train, label=y_train, cat_features=cat_features)\n",
    "test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations=10,\n",
    "    learning_rate=1,\n",
    "    depth=2,\n",
    "    loss_function='MultiClass'\n",
    ")\n",
    "\n",
    "model.fit(train_pool)\n",
    "y_pred = model.predict(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/ np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imptce_df = pd.DataFrame({\n",
    "    'feature_importance': model.get_feature_importance(train_pool), \n",
    "    'feature_names': X_train.columns\n",
    "}).sort_values('feature_importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_imptce_df.to_csv(output_filename, index=False)\n",
    "feature_imptce_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SingleClass (N-clusters times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imptce_df = pd.DataFrame()\n",
    "column_name = f'{model_nm}_{n_clusters}'\n",
    "X, y = df.drop(columns=[column_name]), df[column_name]\n",
    "\n",
    "for label in tqdm(range(n_clusters)):\n",
    "    new_y_labels = [0 if _ != label else 1 for _ in range(n_clusters)]\n",
    "    new_y_labels_dict = dict(zip([str(_) for _ in range(n_clusters)], new_y_labels))\n",
    "\n",
    "    y_single = y.map(new_y_labels_dict)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_single, test_size=0.33, random_state=123, stratify=y_single)\n",
    "\n",
    "    cat_features = X_test.select_dtypes(include=['category','object']).columns.tolist()\n",
    "\n",
    "    train_pool = Pool(data=X_train, label=y_train, cat_features=cat_features)\n",
    "    test_pool = Pool(data=X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "    model = CatBoostClassifier(iterations=10,\n",
    "                           learning_rate=1,\n",
    "                           depth=2,\n",
    "                           verbose=False)\n",
    "\n",
    "    model.fit(train_pool)\n",
    "    y_pred = model.predict(test_pool)\n",
    "\n",
    "    temp_feature_imptce_df = pd.DataFrame({\n",
    "        'label':label,\n",
    "        'feature_importance': model.get_feature_importance(train_pool), \n",
    "        'feature_names': X_train.columns\n",
    "    }).sort_values('feature_importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    feature_imptce_df = pd.concat([feature_imptce_df, temp_feature_imptce_df])\n",
    "\n",
    "print(feature_imptce_df['label'].nunique())\n",
    "feature_imptce_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imptce_df.to_csv(output_filename, index=False)\n",
    "feature_imptce_df.to_clipboard(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_imptce_df[feature_imptce_df['feature_importance'] > 0]['feature_names'].size)\n",
    "feature_imptce_df[feature_imptce_df['feature_importance'] > 0]['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df = pd.DataFrame()\n",
    "\n",
    "for model_ in [_ for _ in df.columns if model_nm in _]:\n",
    "    tmp = df.groupby(model_, as_index=False).agg({'ID':'nunique','Income':'sum'})\n",
    "    tmp['user_pct'] = tmp['ID'] / tmp['ID'].sum() * 100\n",
    "    tmp['income_pct'] = tmp['Income'] / tmp['Income'].sum() * 100\n",
    "    tmp['income_per_user'] = tmp['Income'] / tmp['ID']\n",
    "    tmp['model'] = model_\n",
    "    tmp.columns = ['cluster','user_cnt','Income','user_pct','income_pct','income_per_user','model']\n",
    "\n",
    "    stat_df = pd.concat([stat_df, tmp[['model','cluster','user_cnt','user_pct','Income','income_pct','income_per_user']]])\n",
    "\n",
    "stat_df.to_clipboard(index=False)\n",
    "stat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = ['Income', 'Kidhome','Teenhome', 'Recency', 'Year_Birth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(metric_cols)\n",
    "y = column_name\n",
    "order = df[y].sort_values().unique().tolist()\n",
    "fig, ax = plt.subplots(n,1,figsize=(8, 2*n))\n",
    "c=0\n",
    "\n",
    "for f in metric_cols:\n",
    "    sns.boxplot(data=df, x=f, y=y, order = order, palette = 'viridis', showfliers=False, ax=ax[c])\n",
    "    ax[c].set(xlabel='',ylabel='')\n",
    "    ax[c].set_title(f, loc='left')\n",
    "    c+=1\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.savefig(f'{output_path}{y}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Income'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.groupby([df[feature]==1, y], as_index=False).size()\n",
    "# t = df.groupby([feature, y], as_index=False).size()\n",
    "# t = df.groupby([df[feature].isin(range(90)), y], as_index=False).size()\n",
    "\n",
    "t['part'] = t['size'] / t.groupby(y)['size'].transform(sum) * 100\n",
    "t.drop(columns='size').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs = [.1,.2,.25,.3,.4,.5,.6,.7,.75,.8,.85,.9,.95,.99]\n",
    "print(feature)\n",
    "df.groupby(y)[feature].describe(percentiles=percs).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Marital_Status_Divorced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(8, 2))\n",
    "\n",
    "ax = sns.boxplot(data=df, x=feature, y=column_name, order = order, palette = 'viridis', showfliers=False)\n",
    "ax.set(xlabel='',ylabel='')\n",
    "ax.set_title(feature, loc='left')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percs = [.1,.2,.25,.3,.4,.5,.6,.7,.75,.8,.85,.9,.95,.99]\n",
    "print(feature)\n",
    "df.groupby(column_name)[feature].describe(percentiles=percs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = df.groupby([feature, 'cluster'], as_index=False).size()\n",
    "t = df.groupby([df[feature]>0, column_name], as_index=False).size()\n",
    "\n",
    "t['part'] = t['size'] / t.groupby(column_name)['size'].transform(sum) * 100\n",
    "t.drop(columns='size').T #.to_clipboard()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
